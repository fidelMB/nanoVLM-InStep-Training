{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l7x7SCV-IMQJ"
      },
      "source": [
        "# Create datasets with question templates\n",
        "Enter a hugging face dataset and create a new dataset with the question template with help from gemini to train the nano vlm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9xul8raI95u",
        "outputId": "cb95eb5b-643e-4aeb-aafe-6cd95194f821"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cloning into 'mimic-cxr-dataset'...\n",
            "remote: Enumerating objects: 12, done.\u001b[K\n",
            "remote: Counting objects: 100% (9/9), done.\u001b[K\n",
            "remote: Compressing objects: 100% (9/9), done.\u001b[K\n",
            "remote: Total 12 (delta 1), reused 0 (delta 0), pack-reused 3 (from 1)\u001b[K\n",
            "Unpacking objects: 100% (12/12), 3.62 KiB | 1.21 MiB/s, done.\n",
            "Filtering content: 100% (2/2), 756.16 MiB | 44.54 MiB/s, done.\n"
          ]
        }
      ],
      "source": [
        "!git clone https://huggingface.co/datasets/itsanmolgupta/mimic-cxr-dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MuzovNVJJQGl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from google import genai\n",
        "import getpass"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ueeLXDxhJJVl"
      },
      "outputs": [],
      "source": [
        "df = pd.read_parquet(\"mimic-cxr-dataset/data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uu-QuXtPKxCE",
        "outputId": "601bb80d-66be-4dde-f2b1-3b118d7ba601"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "··········\n"
          ]
        }
      ],
      "source": [
        "client = genai.Client(api_key=getpass.getpass())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "cdOSW763Qbkn"
      },
      "outputs": [],
      "source": [
        "def call_gemini(source_type, text):\n",
        "  prompt_template = '''\n",
        "  You are a board-certified radiologist helping to train medical students in interpreting chest X-rays.\n",
        "\n",
        "  Given the following {source_type} text from a radiology report:\n",
        "\n",
        "  \"{text}\"\n",
        "\n",
        "  Generate a **multiple-choice question** related to the key medical insights or diagnoses implied by this text. The question should test clinical understanding or interpretation.\n",
        "\n",
        "  **Instructions:**\n",
        "  - Ask one clear, focused question based on the text\n",
        "  - Provide **4 distinct answer choices** and include them inside the question string of the JSON and separate them in an A), B), C), D) format\n",
        "  - Clearly mark the **correct answer**\n",
        "  - Ensure the question makes sense without needing the image, but reflects the medical content of the findings\n",
        "  - The information needed to conclude the right answer NEEDS to be inside the given text, do not assume anything that is not inside the text\n",
        "\n",
        "  **Return your output in the following JSON format:**\n",
        "\n",
        "  ```\n",
        "  {{\n",
        "    \"question\": \"...\",\n",
        "    \"answer\": \"...\"\n",
        "  }}\n",
        "  '''\n",
        "\n",
        "  filled_prompt = prompt_template.format(\n",
        "    source_type = source_type,\n",
        "    text = text\n",
        "  )\n",
        "\n",
        "  response = client.models.generate_content(\n",
        "    model=\"gemini-2.0-flash\", contents=filled_prompt\n",
        "  )\n",
        "\n",
        "  text = response.text\n",
        "\n",
        "  if \"```json\" in text:\n",
        "    start = text.index(\"```json\") + len(\"```json\")\n",
        "    end = text.index(\"```\", start)\n",
        "    json_str = text[start:end].strip()\n",
        "    return json_str\n",
        "  else:\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 94,
      "metadata": {
        "id": "weRi7v4ONVJZ"
      },
      "outputs": [],
      "source": [
        "dataset = []\n",
        "\n",
        "for row in df.head().itertuples():\n",
        "  # generate and add question of impressions to dataset\n",
        "  dataset.append({\n",
        "    \"image\": row.image,\n",
        "    \"texts\": call_gemini(\"impression\", row.impression)\n",
        "  })\n",
        "\n",
        "  # generate and add question of findings to dataset\n",
        "  dataset.append({\n",
        "    \"image\": row.image,\n",
        "    \"texts\": call_gemini(\"findings\", row.findings)\n",
        "  })\n",
        "\n",
        "final_df = pd.DataFrame(dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 107,
      "metadata": {
        "id": "kDqKhtJfaetz"
      },
      "outputs": [],
      "source": [
        "final_df.to_parquet(\"data.parquet\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
